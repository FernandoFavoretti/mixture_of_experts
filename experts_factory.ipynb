{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Expert():\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return (1.) / (1 + np.exp(-x))\n",
    "\n",
    "    \n",
    "    def sigmoid_derivativa(self, x):\n",
    "        return self.sigmoid(x) * (1 - self.sigmoid(x))\n",
    "\n",
    "    \n",
    "    def tan_hiperbolica(self, x):\n",
    "        #from https://alexander-wong.com/post/neural-networks-and-deep-learning-week3/\n",
    "        return (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))\n",
    "\n",
    "    \n",
    "    def tan_hiperbolica_derivativa(self, x):\n",
    "        #from https://alexander-wong.com/post/neural-networks-and-deep-learning-week3/\n",
    "        return 1 - self.tan_hiperbolica(x)**2\n",
    "    \n",
    "    \n",
    "    def softmax(self,x):\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum()\n",
    "    \n",
    "    \n",
    "    def softmax_derivativa(self,x):\n",
    "        '''\n",
    "        Retirada do codigo disponibilizado em aula ` dYdYin(:,cont)=(1-Y(:,i)).*Y(:,i);%sigmoid e softamax`\n",
    "        '''\n",
    "        return  np.multiply((1 - self.softmax(x)),self.softmax(x))\n",
    "    \n",
    "    \n",
    "    def __init__(self,ne,nh,ns,g_h,g_o,max_epoch,alpha):\n",
    "        '''\n",
    "        ne = numero de neuronios entradas\n",
    "        nh = numero neuronios hidden layer\n",
    "        ns = numero de neuronios saida\n",
    "        \n",
    "        As funcoes de ativacao sao denotadas por\n",
    "        g_h = funcao de ativacao camada escondida\n",
    "        g_o = funcao de ativacao camada de saida\n",
    "        \n",
    "        e podem receber as seguintes entradas\n",
    "        'sigmoid', 'tan_h', 'softmax' e 'linear'\n",
    "        '''\n",
    "        self.W1=np.random.uniform(size=(ne,nh))\n",
    "        self.b1=np.random.uniform(size=(1,nh))\n",
    "        self.W2=np.random.uniform(size=(nh,ns))\n",
    "        self.b2=np.random.uniform(size=(1,ns))\n",
    "        self.ghidden = g_h\n",
    "        self.gout = g_o\n",
    "        self.max_epoch = max_epoch\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def executa_funcao_ativacao(self, tipo, x, derivativa=False):\n",
    "        if tipo == 'sigmoid':\n",
    "            if derivativa:\n",
    "                return self.sigmoid_derivativa(x)\n",
    "            else:\n",
    "                return self.sigmoid(x)\n",
    "        elif tipo == 'tan_h':\n",
    "            if derivativa:\n",
    "                return self.tan_hiperbolica_derivativa(x)\n",
    "            else:\n",
    "                return self.tan_hiperbolica(x)\n",
    "        elif tipo == 'softmax':\n",
    "            if derivativa:\n",
    "                return self.softmax_derivativa(x)\n",
    "            else:\n",
    "                return self.softmax(x)\n",
    "        elif tipo == 'linear':\n",
    "            if derivativa:\n",
    "                return np.ones(x.shape)\n",
    "            else:\n",
    "                return x\n",
    "    \n",
    "    \n",
    "    def calcula_loss(self,erro_epoca):\n",
    "        '''\n",
    "        Calcula EQM\n",
    "        '''\n",
    "        loss = np.square(erro_epoca).mean()\n",
    "        return loss\n",
    "            \n",
    "            \n",
    "    def feedforward(self, X):\n",
    "        #Forward Propogation\n",
    "        #entrada -> hidden\n",
    "        z1 = np.dot(X,self.W1) + self.b1\n",
    "        a1 = self.executa_funcao_ativacao(self.ghidden, z1)\n",
    "        #hidden -> saida\n",
    "        z2=np.dot(a1,self.W2)+ self.b2\n",
    "        output = self.executa_funcao_ativacao(self.gout, z2)\n",
    "        return z1,a1,z2,output\n",
    "    \n",
    "    \n",
    "    def backpropagation(self, X,y,z1,a1,z2,output,alpha,h_tipo,h):\n",
    "        #Calcula deltas por partes\n",
    "        if h_tipo == 'exp':\n",
    "            ns=output.shape[0]\n",
    "            e = y - output\n",
    "        if h_tipo == 'gat':\n",
    "            e = h - output\n",
    "            \n",
    "        erro_epoca = np.multiply(h,e)\n",
    "        delta_output = self.executa_funcao_ativacao(self.gout, output, derivativa=True)\n",
    "        d_z2 = erro_epoca * delta_output\n",
    "        \n",
    "        erro_hidden = d_z2.dot(self.W2.T) * (1/len(X))\n",
    "        delta_hidden = self.executa_funcao_ativacao(self.ghidden,z1,derivativa=True)\n",
    "        d_z1 = erro_hidden * delta_hidden * (1/len(X))\n",
    "        \n",
    "        #Atualiza pesos\n",
    "        self.W2 += z1.T.dot(d_z2) * alpha\n",
    "        self.b2 += np.sum(d_z2, axis=0,keepdims=True) * alpha\n",
    "        self.W1 += X.T.dot(d_z1) * alpha\n",
    "        self.b1 +=np.sum(d_z1, axis=0,keepdims=True) * alpha\n",
    "        return erro_epoca\n",
    "    \n",
    "\n",
    "    def train(self, X, y, X_val, y_val, numero_max_erro_val=10,h_tipo='exp',h=1,plot=False):\n",
    "        #Variaveis de controle\n",
    "        all_losses = [] #para plot\n",
    "        numero_erro_validacao_subiu = 0 #acompanhamento do erro de validacao\n",
    "        last_eqm_val = 99999  #acompanhamento do erro de validacao\n",
    "        max_epoch = self.max_epoch\n",
    "        alpha = self.alpha\n",
    "        #armazena melhores pesos para retorno posterior\n",
    "        melhores_pesos = {\n",
    "            'W1' : self.W1,\n",
    "            'b1' : self.b1,\n",
    "            'W2' : self.W2,\n",
    "            'b2' : self.b2\n",
    "        }\n",
    "        #Itera sobre as epocas\n",
    "        for epoch in range(1,max_epoch+1):\n",
    "            #feedforward\n",
    "            z1,a1,z2,output = self.feedforward(X)\n",
    "            #backward\n",
    "            erro_epoca = self.backpropagation(X,y,z1,a1,z2,output,alpha,h_tipo,h)\n",
    "            #gera loss\n",
    "            all_losses.append(self.calcula_loss(erro_epoca))\n",
    "            \n",
    "            #calculo para acompanhar erro no conjunto de validacao\n",
    "            z1_val,a1_val,z2_val,output_val = self.feedforward(X_val)\n",
    "            #calcula erro de validacao\n",
    "            erro_validacao = output_val - y_val\n",
    "            eqm_validacao = self.calcula_loss(erro_validacao)\n",
    "            \n",
    "            #Acompanhamento eveolucao do erro no conjunto de validacao\n",
    "            if eqm_validacao < last_eqm_val:\n",
    "                numero_erro_validacao_subiu = 0\n",
    "                last_eqm_val = eqm_validacao\n",
    "                #Atualiza melhores pesos\n",
    "                melhores_pesos = {\n",
    "                    'W1' : self.W1,\n",
    "                    'b1' : self.b1,\n",
    "                    'W2' : self.W2,\n",
    "                    'b2' : self.b2\n",
    "                }\n",
    "            else:\n",
    "                last_eqm_val = eqm_validacao\n",
    "                numero_erro_validacao_subiu += 1\n",
    "                if numero_erro_validacao_subiu >= numero_max_erro_val:\n",
    "#                     print(\"Treinamento encerrado por aumentos consecutivos no erro de validacao, epocas {}\".format(epoch))\n",
    "                    #retorna os melhores pesos\n",
    "                    self.W1 = melhores_pesos['W1']\n",
    "                    self.b1 = melhores_pesos['b1']\n",
    "                    self.W2 = melhores_pesos['W2']\n",
    "                    self.b2 = melhores_pesos['b2']\n",
    "                    if plot:       \n",
    "                        #Se deseja plotar a evolucao do erro\n",
    "                        import matplotlib.pyplot as plt\n",
    "                        plt.plot(all_losses)\n",
    "                        plt.show()\n",
    "                    return melhores_pesos,all_losses\n",
    "            \n",
    "#         print(\"Treinamento encerrado em {} epocas\".format(epoch))\n",
    "        if plot:       \n",
    "            import matplotlib.pyplot as plt\n",
    "            plt.plot(all_losses)\n",
    "            plt.show()\n",
    "        return melhores_pesos,all_losses\n",
    "\n",
    "    def predict(self,X,melhores_pesos):\n",
    "        #Retorna melhores pesos\n",
    "        self.W1 = melhores_pesos['W1']\n",
    "        self.b1 = melhores_pesos['b1']\n",
    "        self.W2 = melhores_pesos['W2']\n",
    "        self.b2 = melhores_pesos['b2']\n",
    "        z1,a1,z2,output = self.feedforward(X)\n",
    "        return output\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/home/favoretti/mestrado/data_mining/clods/mixture_of_experts/utils.py'>"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import utils\n",
    "reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import utils\n",
    "#Lendo o dadoa\n",
    "df = pd.read_csv('data/treinamento-1.txt', header=None)\n",
    "df = utils.normalize_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "Tamanho total 985\n",
      "Tamanho treino 689\n",
      "Tamanho teste 197\n",
      "Tamanho validacao 99\n",
      "==============\n"
     ]
    }
   ],
   "source": [
    "num_lags = 15\n",
    "\n",
    "#criando Lag\n",
    "lagged_data = utils.create_lag(df, num_lags)\n",
    "lagged_data = lagged_data.reset_index(drop=True)\n",
    "\n",
    "X = lagged_data.drop(['y'],axis=1)\n",
    "y = lagged_data['y']\n",
    "\n",
    "#Criando conjunto de dados\n",
    "fracao_dados_para_treino = 0.7\n",
    "fracao_dados_para_teste = 0.2\n",
    "X_train,y_train,X_test,y_test,X_val,y_val = utils.treino_teste_validacao(X,y, frac_train=fracao_dados_para_treino, frac_test=fracao_dados_para_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variable initialization\n",
    "max_epocas= 6000\n",
    "alpha=0.6 #Setting learning rate\n",
    "ne = X.shape[1] #numero de features no dataset\n",
    "nh = 5 #numero neuronios hidden\n",
    "ns = 1 #numero neuronios saida\n",
    "ep =Expert(ne,nh,ns,g_h='sigmoid',g_o='sigmoid',max_epoch=max_epocas, alpha=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/favoretti/.local/lib/python3.5/site-packages/ipykernel_launcher.py:5: RuntimeWarning: overflow encountered in exp\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.32905537827096615"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "melhores_pesos,all_losses = ep.train(X_train, y_train, X_val, y_val, 50, plot=False)\n",
    "np.mean(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotnine import *\n",
    "df_loss = pd.DataFrame(all_losses, columns=[\"eqm\"])\n",
    "df_loss = df_loss.reset_index()\n",
    "df_loss = df_loss.rename({'index':'epoca'},axis=1)\n",
    "\n",
    "ggplot(df_loss, aes(y = 'eqm', x='epoca'))\\\n",
    "+ geom_line(alpha=0.6, color='lightblue')\\\n",
    "+ geom_smooth(color='purple')\\\n",
    "+ ggtitle(\"Evolucao do Erro para especialista\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
