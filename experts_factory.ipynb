{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Expert():\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return (1.) / (1 + np.exp(-x))\n",
    "\n",
    "    \n",
    "    def sigmoid_derivativa(self, x):\n",
    "        return self.sigmoid(x) * (1 - self.sigmoid(x))\n",
    "\n",
    "    \n",
    "    def tan_hiperbolica(self, x):\n",
    "        #from https://alexander-wong.com/post/neural-networks-and-deep-learning-week3/\n",
    "        return (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))\n",
    "\n",
    "    \n",
    "    def tan_hiperbolica_derivativa(self, x):\n",
    "        #from https://alexander-wong.com/post/neural-networks-and-deep-learning-week3/\n",
    "        return 1 - self.tan_hiperbolica(x)**2\n",
    "    \n",
    "    \n",
    "    def softmax(self,x):\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum()\n",
    "    \n",
    "    \n",
    "    def softmax_derivativa(self,x):\n",
    "        '''\n",
    "        Retirada do codigo disponibilizado em aula ` dYdYin(:,cont)=(1-Y(:,i)).*Y(:,i);%sigmoid e softamax`\n",
    "        '''\n",
    "        return  np.multiply((1 - self.softmax(x)),self.softmax(x))\n",
    "    \n",
    "    \n",
    "    def __init__(self,ne,nh,ns,g_h,g_o):\n",
    "        '''\n",
    "        ne = numero de neuronios entradas\n",
    "        nh = numero neuronios hidden layer\n",
    "        ns = numero de neuronios saida\n",
    "        \n",
    "        As funcoes de ativacao sao denotadas por\n",
    "        g_h = funcao de ativacao camada escondida\n",
    "        g_o = funcao de ativacao camada de saida\n",
    "        \n",
    "        e podem receber as seguintes entradas\n",
    "        'sigmoid', 'tan_h', 'softmax' e 'linear'\n",
    "        '''\n",
    "        self.W1=np.random.uniform(size=(ne,nh))\n",
    "        self.b1=np.random.uniform(size=(1,nh))\n",
    "        self.W2=np.random.uniform(size=(nh,ns))\n",
    "        self.b2=np.random.uniform(size=(1,ns))\n",
    "        self.ghidden = g_h\n",
    "        self.gout = g_o\n",
    "        \n",
    "        \n",
    "    def executa_funcao_ativacao(self, tipo, x, derivativa=False):\n",
    "        if tipo == 'sigmoid':\n",
    "            if derivativa:\n",
    "                return self.sigmoid_derivativa(x)\n",
    "            else:\n",
    "                return self.sigmoid(x)\n",
    "        elif tipo == 'tan_h':\n",
    "            if derivativa:\n",
    "                return self.tan_hiperbolica_derivativa(x)\n",
    "            else:\n",
    "                return self.tan_hiperbolica(x)\n",
    "        elif tipo == 'softmax':\n",
    "            if derivativa:\n",
    "                return self.softmax_derivativa(x)\n",
    "            else:\n",
    "                return self.softmax(x)\n",
    "        elif tipo == 'linear':\n",
    "            if derivativa:\n",
    "                return np.ones(x.shape)\n",
    "            else:\n",
    "                return x\n",
    "    \n",
    "    \n",
    "    def calcula_loss(self,erro_epoca):\n",
    "        '''\n",
    "        Calcula EQM\n",
    "        '''\n",
    "        loss = np.square(erro_epoca).mean()\n",
    "        return loss\n",
    "            \n",
    "            \n",
    "    def feedforward(self, X):\n",
    "        #Forward Propogation\n",
    "        #entrada -> hidden\n",
    "        z1 = np.dot(X,self.W1) + self.b1\n",
    "        a1 = self.executa_funcao_ativacao(self.ghidden, z1)\n",
    "        #hidden -> saida\n",
    "        z2=np.dot(a1,self.W2)+ self.b2\n",
    "        output = self.executa_funcao_ativacao(self.gout, z2)\n",
    "        return z1,a1,z2,output\n",
    "    \n",
    "    \n",
    "    def backpropagation(self, X,y,z1,a1,z2,output,alpha,h_tipo,h):\n",
    "        #Calcula deltas por partes\n",
    "        if h_tipo == 'exp':\n",
    "            ns=output.shape[0]\n",
    "            h = np.dot(h,np.ones((ns,1)))\n",
    "            e = y - output\n",
    "        if h_tipo == 'gat':\n",
    "            e = h - output\n",
    "        erro_epoca = np.multiply(h,e)\n",
    "        delta_output = self.executa_funcao_ativacao(self.gout, output, derivativa=True)\n",
    "        d_z2 = erro_epoca * delta_output\n",
    "        erro_hidden = d_z2.dot(self.W2.T) * (1/len(X))\n",
    "        delta_hidden = self.executa_funcao_ativacao(self.ghidden,z1,derivativa=True)\n",
    "        d_z1 = erro_hidden * delta_hidden * (1/len(X))\n",
    "        \n",
    "        #Atualiza pesos\n",
    "        self.W2 += z1.T.dot(d_z2) * alpha\n",
    "        self.b2 += np.sum(d_z2, axis=0,keepdims=True) * alpha\n",
    "        self.W1 += X.T.dot(d_z1) * alpha\n",
    "        self.b1 +=np.sum(d_z1, axis=0,keepdims=True) * alpha\n",
    "        return erro_epoca\n",
    "    \n",
    "\n",
    "    def train(self, max_epoch, alpha, X, y, X_val, y_val, numero_max_erro_val=10,h_tipo='exp',h=1,plot=False):\n",
    "        #Variaveis de controle\n",
    "        all_losses = [] #para plot\n",
    "        numero_erro_validacao_subiu = 0 #acompanhamento do erro de validacao\n",
    "        last_eqm_val = 99999  #acompanhamento do erro de validacao\n",
    "\n",
    "        #armazena melhores pesos para retorno posterior\n",
    "        melhores_pesos = {\n",
    "            'W1' : self.W1,\n",
    "            'b1' : self.b1,\n",
    "            'W2' : self.W2,\n",
    "            'b2' : self.b2\n",
    "        }\n",
    "        #Itera sobre as epocas\n",
    "        for epoch in range(1,max_epoch+1):\n",
    "            #feedforward\n",
    "            z1,a1,z2,output = self.feedforward(X)\n",
    "            #backward\n",
    "            erro_epoca = self.backpropagation(X,y,z1,a1,z2,output,alpha,h_tipo,h)\n",
    "            #gera loss\n",
    "            all_losses.append(self.calcula_loss(erro_epoca))\n",
    "            \n",
    "            #calculo para acompanhar erro no conjunto de validacao\n",
    "            z1_val,a1_val,z2_val,output_val = self.feedforward(X_val)\n",
    "            #calcula erro de validacao\n",
    "            erro_validacao = output_val - y_val\n",
    "            eqm_validacao = self.calcula_loss(erro_validacao)\n",
    "            \n",
    "            #Acompanhamento eveolucao do erro no conjunto de validacao\n",
    "            if eqm_validacao < last_eqm_val:\n",
    "                numero_erro_validacao_subiu = 0\n",
    "                last_eqm_val = eqm_validacao\n",
    "                #Atualiza melhores pesos\n",
    "                melhores_pesos = {\n",
    "                    'W1' : self.W1,\n",
    "                    'b1' : self.b1,\n",
    "                    'W2' : self.W2,\n",
    "                    'b2' : self.b2\n",
    "                }\n",
    "            else:\n",
    "                last_eqm_val = eqm_validacao\n",
    "                numero_erro_validacao_subiu += 1\n",
    "                if numero_erro_validacao_subiu >= numero_max_erro_val:\n",
    "                    print(\"Treinamento encerrado por aumentos consecutivos no erro de validacao, epocas {}\".format(epoch))\n",
    "                    #retorna os melhores pesos\n",
    "                    self.W1 = melhores_pesos['W1']\n",
    "                    self.b1 = melhores_pesos['b1']\n",
    "                    self.W2 = melhores_pesos['W2']\n",
    "                    self.b2 = melhores_pesos['b2']\n",
    "                    if plot:       \n",
    "                        #Se deseja plotar a evolucao do erro\n",
    "                        import matplotlib.pyplot as plt\n",
    "                        plt.plot(all_losses)\n",
    "                        plt.show()\n",
    "                    return melhores_pesos,all_losses\n",
    "            \n",
    "        print(\"Treinamento encerrado em {} epocas\".format(epoch))\n",
    "        if plot:       \n",
    "            import matplotlib.pyplot as plt\n",
    "            plt.plot(all_losses)\n",
    "            plt.show()\n",
    "        return melhores_pesos,all_losses\n",
    "\n",
    "    def predict(self,X,melhores_pesos):\n",
    "        #Retorna melhores pesos\n",
    "        self.W1 = melhores_pesos['W1']\n",
    "        self.b1 = melhores_pesos['b1']\n",
    "        self.W2 = melhores_pesos['W2']\n",
    "        self.b2 = melhores_pesos['b2']\n",
    "        z1,a1,z2,output = self.feedforward(X)\n",
    "        return output\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/home/favoretti/mestrado/data_mining/clods/mixture_of_experts/utils.py'>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import utils\n",
    "reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import utils\n",
    "#Lendo o dadoa\n",
    "df = pd.read_csv('data/treinamento-1.txt', header=None)\n",
    "df = utils.normalize_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "Tamanho total 985\n",
      "Tamanho treino 689\n",
      "Tamanho teste 197\n",
      "Tamanho validacao 99\n",
      "==============\n"
     ]
    }
   ],
   "source": [
    "num_lags = 15\n",
    "\n",
    "#criando Lag\n",
    "lagged_data = utils.create_lag(df, num_lags)\n",
    "lagged_data = lagged_data.reset_index(drop=True)\n",
    "\n",
    "X = lagged_data.drop(['y'],axis=1)\n",
    "y = lagged_data['y']\n",
    "\n",
    "#Criando conjunto de dados\n",
    "fracao_dados_para_treino = 0.7\n",
    "fracao_dados_para_teste = 0.2\n",
    "X_train,y_train,X_test,y_test,X_val,y_val = utils.treino_teste_validacao(X,y, frac_train=fracao_dados_para_treino, frac_test=fracao_dados_para_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variable initialization\n",
    "max_epocas= 1000\n",
    "alpha=0.005 #Setting learning rate\n",
    "ne = X.shape[1] #numero de features no dataset\n",
    "nh = 5 #numero neuronios hidden\n",
    "ns = 2 #numero neuronios saida\n",
    "ep =Expert(ne,nh,ns,g_h='sigmoid',g_o='sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinamento encerrado em 1000 epocas\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9f3c8661d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.23723373308295043"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "melhores_pesos,all_losses = ep.train(max_epocas, alpha, X_train, y_train, X_val, y_val, 50, plot=True)\n",
    "np.min(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
