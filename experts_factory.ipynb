{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Expert():\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    \n",
    "    def sigmoid_derivativa(self, x):\n",
    "        return self.sigmoid(x) * (1 - self.sigmoid(x))\n",
    "\n",
    "    \n",
    "    def tan_hiperbolica(self, x):\n",
    "        #from https://alexander-wong.com/post/neural-networks-and-deep-learning-week3/\n",
    "        return (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))\n",
    "\n",
    "    \n",
    "    def tan_hiperbolica_derivativa(self, x):\n",
    "        #from https://alexander-wong.com/post/neural-networks-and-deep-learning-week3/\n",
    "        return 1 - self.tan_hiperbolica(x)**2\n",
    "    \n",
    "    \n",
    "    def softmax(self,x):\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum()\n",
    "    \n",
    "    \n",
    "    def softmax_derivativa(self,x):\n",
    "        '''\n",
    "        Retirada do codigo disponibilizado em aula ` dYdYin(:,cont)=(1-Y(:,i)).*Y(:,i);%sigmoid e softamax`\n",
    "        '''\n",
    "        return  np.multiply((1 - self.softmax(x)),self.softmax(x))\n",
    "    \n",
    "    \n",
    "    def __init__(self,ne,nh,ns,g_h,g_o):\n",
    "        '''\n",
    "        ne = numero de neuronios entradas\n",
    "        nh = numero neuronios hidden layer\n",
    "        ns = numero de neuronios saida\n",
    "        \n",
    "        As funcoes de ativacao sao denotadas por\n",
    "        g_h = funcao de ativacao camada escondida\n",
    "        g_o = funcao de ativacao camada de saida\n",
    "        \n",
    "        e podem receber as seguintes entradas\n",
    "        'sigmoid', 'tan_h', 'softmax' e 'linear'\n",
    "        '''\n",
    "        self.W1=np.random.uniform(size=(ne,nh))\n",
    "        self.b1=np.random.uniform(size=(1,nh))\n",
    "        self.W2=np.random.uniform(size=(nh,ns))\n",
    "        self.b2=np.random.uniform(size=(1,ns))\n",
    "        self.ghidden = g_h\n",
    "        self.gout = g_o\n",
    "        \n",
    "        \n",
    "    def executa_funcao_ativacao(self, tipo, x, derivativa=False):\n",
    "        if tipo == 'sigmoid':\n",
    "            if derivativa:\n",
    "                return self.sigmoid_derivativa(x)\n",
    "            else:\n",
    "                return self.sigmoid(x)\n",
    "        elif tipo == 'tan_h':\n",
    "            if derivativa:\n",
    "                return self.tan_hiperbolica_derivativa(x)\n",
    "            else:\n",
    "                return self.tan_hiperbolica(x)\n",
    "        elif tipo == 'softmax':\n",
    "            if derivativa:\n",
    "                return self.softmax_derivativa(x)\n",
    "            else:\n",
    "                return self.softmax(x)\n",
    "        elif tipo == 'linear':\n",
    "            if derivativa:\n",
    "                return np.ones(x.shape)\n",
    "            else:\n",
    "                return x\n",
    "    \n",
    "    \n",
    "    def calcula_loss(self,erro_epoca):\n",
    "        '''\n",
    "        Calcula EQM\n",
    "        '''\n",
    "        loss = np.square(erro_epoca).mean()\n",
    "        return loss\n",
    "            \n",
    "            \n",
    "    def feedforward(self, X):\n",
    "        #Forward Propogation\n",
    "        #entrada -> hidden\n",
    "        z1 = np.dot(X,self.W1) + self.b1\n",
    "        a1 = self.executa_funcao_ativacao(self.ghidden, z1)\n",
    "        #hidden -> saida\n",
    "        z2=np.dot(a1,self.W2)+ self.b2\n",
    "        output = self.executa_funcao_ativacao(self.gout, z2)\n",
    "        return z1,a1,z2,output\n",
    "    \n",
    "    \n",
    "    def backpropagation(self, X,y,z1,a1,z2,output,alpha):\n",
    "        #Calcula deltas por partes\n",
    "        erro_epoca = output - y\n",
    "        delta_output = self.executa_funcao_ativacao(self.gout, output, derivativa=True)\n",
    "        d_z2 = erro_epoca * delta_output\n",
    "        erro_hidden = d_z2.dot(self.W2.T) * (1/len(X))\n",
    "        delta_hidden = self.executa_funcao_ativacao(self.ghidden,z1,derivativa=True)\n",
    "        d_z1 = erro_hidden * delta_hidden * (1/len(X))\n",
    "        \n",
    "        #Atualiza pesos\n",
    "        self.W2 -= z1.T.dot(d_z2) * alpha\n",
    "        self.b2 -= np.sum(d_z2, axis=0,keepdims=True) * alpha\n",
    "        self.W1 -= X.T.dot(d_z1) * alpha\n",
    "        self.b1 -= np.sum(d_z1, axis=0,keepdims=True) * alpha\n",
    "        return erro_epoca\n",
    "    \n",
    "\n",
    "    def train(self, max_epoch, alpha, X, y, X_val, y_val, numero_max_erro_val=10, plot=False):\n",
    "        #Variaveis de controle\n",
    "        all_losses = [] #para plot\n",
    "        numero_erro_validacao_subiu = 0 #acompanhamento do erro de validacao\n",
    "        last_eqm_val = 99999  #acompanhamento do erro de validacao\n",
    "\n",
    "        #armazena melhores pesos para retorno posterior\n",
    "        melhores_pesos = {\n",
    "            'W1' : self.W1,\n",
    "            'b1' : self.b1,\n",
    "            'W2' : self.W2,\n",
    "            'b2' : self.b2\n",
    "        }\n",
    "        #Itera sobre as epocas\n",
    "        for epoch in range(1,max_epoch+1):\n",
    "            #feedforward\n",
    "            z1,a1,z2,output = self.feedforward(X)\n",
    "            #backward\n",
    "            erro_epoca = self.backpropagation(X,y,z1,a1,z2,output,alpha)\n",
    "            #gera loss\n",
    "            all_losses.append(self.calcula_loss(erro_epoca))\n",
    "            \n",
    "            #calculo para acompanhar erro no conjunto de validacao\n",
    "            z1_val,a1_val,z2_val,output_val = self.feedforward(X_val)\n",
    "            #calcula erro de validacao\n",
    "            erro_validacao = output_val - y_val\n",
    "            eqm_validacao = self.calcula_loss(erro_validacao)\n",
    "            \n",
    "            #Acompanhamento eveolucao do erro no conjunto de validacao\n",
    "            if eqm_validacao < last_eqm_val:\n",
    "                numero_erro_validacao_subiu = 0\n",
    "                last_eqm_val = eqm_validacao\n",
    "                #Atualiza melhores pesos\n",
    "                melhores_pesos = {\n",
    "                    'W1' : self.W1,\n",
    "                    'b1' : self.b1,\n",
    "                    'W2' : self.W2,\n",
    "                    'b2' : self.b2\n",
    "                }\n",
    "            else:\n",
    "                last_eqm_val = eqm_validacao\n",
    "                numero_erro_validacao_subiu += 1\n",
    "                if numero_erro_validacao_subiu >= numero_max_erro_val:\n",
    "                    print(\"Treinamento encerrado por aumentos consecutivos no erro de validacao, epocas {}\".format(epoch))\n",
    "                    #retorna os melhores pesos\n",
    "                    self.W1 = melhores_pesos['W1']\n",
    "                    self.b1 = melhores_pesos['b1']\n",
    "                    self.W2 = melhores_pesos['W2']\n",
    "                    self.b2 = melhores_pesos['b2']\n",
    "                    if plot:       \n",
    "                        #Se deseja plotar a evolucao do erro\n",
    "                        import matplotlib.pyplot as plt\n",
    "                        plt.plot(all_losses)\n",
    "                        plt.show()\n",
    "                    return melhores_pesos,all_losses\n",
    "            \n",
    "        print(\"Treinamento encerrado em {} epocas\".format(epoch))\n",
    "        if plot:       \n",
    "            import matplotlib.pyplot as plt\n",
    "            plt.plot(all_losses)\n",
    "            plt.show()\n",
    "        return melhores_pesos,all_losses\n",
    "\n",
    "    def predict(self,X,melhores_pesos):\n",
    "        #Retorna melhores pesos\n",
    "        self.W1 = melhores_pesos['W1']\n",
    "        self.b1 = melhores_pesos['b1']\n",
    "        self.W2 = melhores_pesos['W2']\n",
    "        self.b2 = melhores_pesos['b2']\n",
    "        z1,a1,z2,output = self.feedforward(X)\n",
    "        return output\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/home/favoretti/mestrado/data_mining/clods/mixture_of_experts/utils.py'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import utils\n",
    "reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "Tamanho total 990\n",
      "Tamanho treino 693\n",
      "Tamanho teste 198\n",
      "Tamanho validacao 99\n",
      "==============\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import utils\n",
    "#Lendo o dadoa\n",
    "df = pd.read_csv('data/treinamento-1.txt', header=None)\n",
    "num_lags = 10\n",
    "\n",
    "#criando Lag\n",
    "lagged_data = utils.create_lag(df, num_lags)\n",
    "lagged_data = lagged_data.reset_index(drop=True)\n",
    "\n",
    "X = lagged_data.drop(['y'],axis=1)\n",
    "y = lagged_data['y']\n",
    "\n",
    "#Criando conjunto de dados\n",
    "fracao_dados_para_treino = 0.7\n",
    "fracao_dados_para_teste = 0.2\n",
    "X_train,y_train,X_test,y_test,X_val,y_val = utils.treino_teste_validacao(X,y, frac_train=fracao_dados_para_treino, frac_test=fracao_dados_para_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variable initialization\n",
    "max_epocas=1000\n",
    "alpha=0.2 #Setting learning rate\n",
    "ne = X.shape[1] #numero de features no dataset\n",
    "nh = 3 #numero neuronios hidden\n",
    "ns = 1 #numero neuronios saida\n",
    "ep =Expert(ne,nh,ns,g_h='sigmoid',g_o='sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinamento encerrado por aumentos consecutivos no erro de validacao, epocas 97\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f89c7456b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "melhores_pesos,all_losses = ep.train(max_epocas, alpha, X_train, y_train, X_val, y_val, 50, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
