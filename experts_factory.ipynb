{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Expert():\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return (1.) / (1 + np.exp(-x))\n",
    "\n",
    "    \n",
    "    def sigmoid_derivativa(self, x):\n",
    "        return self.sigmoid(x) * (1 - self.sigmoid(x))\n",
    "\n",
    "    \n",
    "    def tan_hiperbolica(self, x):\n",
    "        #from https://alexander-wong.com/post/neural-networks-and-deep-learning-week3/\n",
    "        return (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))\n",
    "\n",
    "    \n",
    "    def tan_hiperbolica_derivativa(self, x):\n",
    "        #from https://alexander-wong.com/post/neural-networks-and-deep-learning-week3/\n",
    "        return 1 - self.tan_hiperbolica(x)**2\n",
    "    \n",
    "    \n",
    "    def softmax(self,x):\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum()\n",
    "    \n",
    "    \n",
    "    def softmax_derivativa(self,x):\n",
    "        '''\n",
    "        Retirada do codigo disponibilizado em aula ` dYdYin(:,cont)=(1-Y(:,i)).*Y(:,i);%sigmoid e softamax`\n",
    "        '''\n",
    "        return  np.multiply((1 - self.softmax(x)),self.softmax(x))\n",
    "    \n",
    "    \n",
    "    def __init__(self,ne,nh,ns,g_h,g_o):\n",
    "        '''\n",
    "        ne = numero de neuronios entradas\n",
    "        nh = numero neuronios hidden layer\n",
    "        ns = numero de neuronios saida\n",
    "        \n",
    "        As funcoes de ativacao sao denotadas por\n",
    "        g_h = funcao de ativacao camada escondida\n",
    "        g_o = funcao de ativacao camada de saida\n",
    "        \n",
    "        e podem receber as seguintes entradas\n",
    "        'sigmoid', 'tan_h', 'softmax' e 'linear'\n",
    "        '''\n",
    "        self.W1=np.random.uniform(size=(ne,nh))\n",
    "        self.b1=np.random.uniform(size=(1,nh))\n",
    "        self.W2=np.random.uniform(size=(nh,ns))\n",
    "        self.b2=np.random.uniform(size=(1,ns))\n",
    "        self.ghidden = g_h\n",
    "        self.gout = g_o\n",
    "        \n",
    "        \n",
    "    def executa_funcao_ativacao(self, tipo, x, derivativa=False):\n",
    "        if tipo == 'sigmoid':\n",
    "            if derivativa:\n",
    "                return self.sigmoid_derivativa(x)\n",
    "            else:\n",
    "                return self.sigmoid(x)\n",
    "        elif tipo == 'tan_h':\n",
    "            if derivativa:\n",
    "                return self.tan_hiperbolica_derivativa(x)\n",
    "            else:\n",
    "                return self.tan_hiperbolica(x)\n",
    "        elif tipo == 'softmax':\n",
    "            if derivativa:\n",
    "                return self.softmax_derivativa(x)\n",
    "            else:\n",
    "                return self.softmax(x)\n",
    "        elif tipo == 'linear':\n",
    "            if derivativa:\n",
    "                return np.ones(x.shape)\n",
    "            else:\n",
    "                return x\n",
    "    \n",
    "    \n",
    "    def calcula_loss(self,erro_epoca):\n",
    "        '''\n",
    "        Calcula EQM\n",
    "        '''\n",
    "        loss = np.square(erro_epoca).mean()\n",
    "        return loss\n",
    "            \n",
    "            \n",
    "    def feedforward(self, X):\n",
    "        #Forward Propogation\n",
    "        #entrada -> hidden\n",
    "        z1 = np.dot(X,self.W1) + self.b1\n",
    "        a1 = self.executa_funcao_ativacao(self.ghidden, z1)\n",
    "        #hidden -> saida\n",
    "        z2=np.dot(a1,self.W2)+ self.b2\n",
    "        output = self.executa_funcao_ativacao(self.gout, z2)\n",
    "        return z1,a1,z2,output\n",
    "    \n",
    "    \n",
    "    def backpropagation(self, X,y,z1,a1,z2,output,alpha,h_tipo,h):\n",
    "        #Calcula deltas por partes\n",
    "        if h_tipo == 'exp':\n",
    "            ns=output.shape[0]\n",
    "            h = np.dot(h,np.ones((ns,1)))\n",
    "            e = y - output\n",
    "        if h_tipo == 'gat':\n",
    "            e = h - output\n",
    "        erro_epoca = np.multiply(h,e)\n",
    "        delta_output = self.executa_funcao_ativacao(self.gout, output, derivativa=True)\n",
    "        d_z2 = erro_epoca * delta_output\n",
    "        erro_hidden = d_z2.dot(self.W2.T) * (1/len(X))\n",
    "        delta_hidden = self.executa_funcao_ativacao(self.ghidden,z1,derivativa=True)\n",
    "        d_z1 = erro_hidden * delta_hidden * (1/len(X))\n",
    "        \n",
    "        #Atualiza pesos\n",
    "        self.W2 -= z1.T.dot(d_z2) * alpha\n",
    "        self.b2 -= np.sum(d_z2, axis=0,keepdims=True) * alpha\n",
    "        self.W1 -= X.T.dot(d_z1) * alpha\n",
    "        self.b1 -= np.sum(d_z1, axis=0,keepdims=True) * alpha\n",
    "        return erro_epoca\n",
    "    \n",
    "\n",
    "    def train(self, max_epoch, alpha, X, y, X_val, y_val, numero_max_erro_val=10,h_tipo='exp',h=1,plot=False):\n",
    "        #Variaveis de controle\n",
    "        all_losses = [] #para plot\n",
    "        numero_erro_validacao_subiu = 0 #acompanhamento do erro de validacao\n",
    "        last_eqm_val = 99999  #acompanhamento do erro de validacao\n",
    "\n",
    "        #armazena melhores pesos para retorno posterior\n",
    "        melhores_pesos = {\n",
    "            'W1' : self.W1,\n",
    "            'b1' : self.b1,\n",
    "            'W2' : self.W2,\n",
    "            'b2' : self.b2\n",
    "        }\n",
    "        #Itera sobre as epocas\n",
    "        for epoch in range(1,max_epoch+1):\n",
    "            #feedforward\n",
    "            z1,a1,z2,output = self.feedforward(X)\n",
    "            #backward\n",
    "            erro_epoca = self.backpropagation(X,y,z1,a1,z2,output,alpha,h_tipo,h)\n",
    "            #gera loss\n",
    "            all_losses.append(self.calcula_loss(erro_epoca))\n",
    "            \n",
    "            #calculo para acompanhar erro no conjunto de validacao\n",
    "            z1_val,a1_val,z2_val,output_val = self.feedforward(X_val)\n",
    "            #calcula erro de validacao\n",
    "            erro_validacao = output_val - y_val\n",
    "            eqm_validacao = self.calcula_loss(erro_validacao)\n",
    "            \n",
    "            #Acompanhamento eveolucao do erro no conjunto de validacao\n",
    "            if eqm_validacao < last_eqm_val:\n",
    "                numero_erro_validacao_subiu = 0\n",
    "                last_eqm_val = eqm_validacao\n",
    "                #Atualiza melhores pesos\n",
    "                melhores_pesos = {\n",
    "                    'W1' : self.W1,\n",
    "                    'b1' : self.b1,\n",
    "                    'W2' : self.W2,\n",
    "                    'b2' : self.b2\n",
    "                }\n",
    "            else:\n",
    "                last_eqm_val = eqm_validacao\n",
    "                numero_erro_validacao_subiu += 1\n",
    "                if numero_erro_validacao_subiu >= numero_max_erro_val:\n",
    "                    print(\"Treinamento encerrado por aumentos consecutivos no erro de validacao, epocas {}\".format(epoch))\n",
    "                    #retorna os melhores pesos\n",
    "                    self.W1 = melhores_pesos['W1']\n",
    "                    self.b1 = melhores_pesos['b1']\n",
    "                    self.W2 = melhores_pesos['W2']\n",
    "                    self.b2 = melhores_pesos['b2']\n",
    "                    if plot:       \n",
    "                        #Se deseja plotar a evolucao do erro\n",
    "                        import matplotlib.pyplot as plt\n",
    "                        plt.plot(all_losses)\n",
    "                        plt.show()\n",
    "                    return melhores_pesos,all_losses\n",
    "            \n",
    "        print(\"Treinamento encerrado em {} epocas\".format(epoch))\n",
    "        if plot:       \n",
    "            import matplotlib.pyplot as plt\n",
    "            plt.plot(all_losses)\n",
    "            plt.show()\n",
    "        return melhores_pesos,all_losses\n",
    "\n",
    "    def predict(self,X,melhores_pesos):\n",
    "        #Retorna melhores pesos\n",
    "        self.W1 = melhores_pesos['W1']\n",
    "        self.b1 = melhores_pesos['b1']\n",
    "        self.W2 = melhores_pesos['W2']\n",
    "        self.b2 = melhores_pesos['b2']\n",
    "        z1,a1,z2,output = self.feedforward(X)\n",
    "        return output\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/home/favoretti/mestrado/data_mining/clods/mixture_of_experts/utils.py'>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import utils\n",
    "reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import utils\n",
    "#Lendo o dadoa\n",
    "df = pd.read_csv('data/treinamento-1.txt', header=None)\n",
    "df = utils.normalize_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "Tamanho total 985\n",
      "Tamanho treino 689\n",
      "Tamanho teste 197\n",
      "Tamanho validacao 99\n",
      "==============\n"
     ]
    }
   ],
   "source": [
    "num_lags = 15\n",
    "\n",
    "#criando Lag\n",
    "lagged_data = utils.create_lag(df, num_lags)\n",
    "lagged_data = lagged_data.reset_index(drop=True)\n",
    "\n",
    "X = lagged_data.drop(['y'],axis=1)\n",
    "y = lagged_data['y']\n",
    "\n",
    "#Criando conjunto de dados\n",
    "fracao_dados_para_treino = 0.7\n",
    "fracao_dados_para_teste = 0.2\n",
    "X_train,y_train,X_test,y_test,X_val,y_val = utils.treino_teste_validacao(X,y, frac_train=fracao_dados_para_treino, frac_test=fracao_dados_para_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variable initialization\n",
    "max_epocas= 1\n",
    "alpha=0.005 #Setting learning rate\n",
    "ne = X.shape[1] #numero de features no dataset\n",
    "nh = 5 #numero neuronios hidden\n",
    "ns = 2 #numero neuronios saida\n",
    "ep =Expert(ne,nh,ns,g_h='sigmoid',g_o='softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinamento encerrado em 1 epocas\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9f73c88ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.36919377654849522"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "melhores_pesos,all_losses = ep.train(max_epocas, alpha, X_train, y_train, X_val, y_val, 50, plot=True)\n",
    "np.min(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
