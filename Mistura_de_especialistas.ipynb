{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import pandas as pd\n",
    "import importlib\n",
    "import expert_factory\n",
    "importlib.reload(expert_factory)\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcula_verossimilhanca(num_experts, y_true, output_experts, output_gating):\n",
    "    py = np.zeros(shape=(num_experts,len(y_true)))\n",
    "    for expert in range(0,num_experts):\n",
    "        for index,y in enumerate(y_true):\n",
    "            #calcula a diff entre o real e o que o cada expert previu\n",
    "            #diff = Ytr(j,:)-Yaux(j,:);\n",
    "            diff = y - output_experts[index,expert]\n",
    "            #Py(j,i)=exp(-diff*diff'/(2*var(i)));   \n",
    "            py[expert,index] = np.exp(np.dot(-diff, diff.T) / (2))\n",
    "    # Likelihood= sum(log(sum(Yg.*Py,2)));\n",
    "    #haux = Yg.*Py;\n",
    "    haux = np.multiply(output_gating, py.T)\n",
    "    likelihood = np.sum(np.log(np.sum(haux,axis=0)))\n",
    "    return likelihood,haux       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_final(X, gating_network, all_experts):\n",
    "    #Calcula saida do gating\n",
    "    gating_output = gating_network.feedforward(X)[-1]\n",
    "    experts_outputs = []\n",
    "    #calcula saida do experts\n",
    "    for exp_net in all_experts:\n",
    "        experts_outputs.append(exp_net.feedforward(X)[-1].tolist())\n",
    "    #retorna o melhor expert como saida\n",
    "    final_output = []\n",
    "    for index_gate, result in enumerate(np.argmax(gating_output,axis=1)):\n",
    "        final_output.append(experts_outputs[result][index_gate])\n",
    "    return final_output\n",
    "        \n",
    "\n",
    "def maximiza_gating(gating_network,max_epocas_gating, alpha_gating, X_train, h, X_val, y_val):\n",
    "    gating_network.train(max_epocas_gating, alpha_gating, X_train, h, X_val, y_val)\n",
    "    \n",
    "def maximiza_expert(expert_network,max_epocas_expert,alpha_expert, X_train, h, X_val, y_val):\n",
    "    expert_network.train(max_epocas_expert, alpha_expert, X_train, h, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "Tamanho total 990\n",
      "Tamanho treino 693\n",
      "Tamanho teste 198\n",
      "Tamanho validacao 99\n",
      "==============\n"
     ]
    }
   ],
   "source": [
    "#Lendo o dadoa\n",
    "df = pd.read_csv('data/treinamento-1.txt', header=None)\n",
    "num_lags = 10\n",
    "\n",
    "#criando Lag\n",
    "lagged_data = utils.create_lag(df, num_lags)\n",
    "lagged_data = lagged_data.reset_index(drop=True)\n",
    "\n",
    "X = lagged_data.drop(['y'],axis=1)\n",
    "y = lagged_data['y']\n",
    "\n",
    "#Criando conjunto de dados\n",
    "fracao_dados_para_treino = 0.7\n",
    "fracao_dados_para_teste = 0.2\n",
    "X_train,y_train,X_test,y_test,X_val,y_val = utils.treino_teste_validacao(X,y, frac_train=fracao_dados_para_treino, frac_test=fracao_dados_para_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vamos comecar com experts iguais\n",
    "exp_ne= X_train.shape[1]\n",
    "exp_nh= 3\n",
    "exp_ns= 1\n",
    "num_experts = 2\n",
    "all_experts = []\n",
    "for _ in range(num_experts):\n",
    "    exp = expert_factory.Expert(exp_ne,exp_nh,exp_ns,g_h='sigmoid',g_o='sigmoid')\n",
    "    all_experts.append(exp)\n",
    "    \n",
    "gating_ne = X_train.shape[1]\n",
    "gating_nh = 3\n",
    "gating_ns = num_experts\n",
    "gating_network = expert_factory.Expert(gating_ne,gating_nh,gating_ns, g_h='sigmoid', g_o='softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.43421965487081726\n",
      "0.4342196548708174\n",
      "0.43421965487081726\n",
      "0.4342196548708174\n",
      "0.4342196548708174\n",
      "0.4342196548708174\n",
      "0.43421965487081743\n",
      "0.43421965487081743\n",
      "0.43421965487081743\n",
      "0.43421965487081743\n",
      "0.43421965487081743\n",
      "0.43421965487081743\n",
      "0.43421965487081743\n",
      "0.43421965487081754\n",
      "0.4342196548708176\n",
      "0.4342196548708176\n",
      "0.4342196548708176\n",
      "0.4342196548708176\n",
      "0.43421965487081776\n",
      "0.43421965487081776\n",
      "0.43421965487081793\n",
      "0.43421965487081793\n",
      "0.43421965487081804\n",
      "0.4342196548708181\n",
      "0.43421965487081826\n",
      "0.43421965487081843\n",
      "0.4342196548708186\n",
      "0.43421965487081876\n",
      "0.434219654870819\n",
      "0.43421965487081926\n",
      "0.4342196548708196\n",
      "0.4342196548708199\n",
      "0.43421965487082026\n",
      "0.4342196548708207\n",
      "0.4342196548708212\n",
      "0.4342196548708218\n",
      "0.4342196548708224\n",
      "0.43421965487082326\n",
      "0.4342196548708242\n",
      "0.43421965487082514\n",
      "0.4342196548708263\n",
      "0.43421965487082764\n",
      "0.43421965487082925\n",
      "0.434219654870831\n",
      "0.434219654870833\n",
      "0.4342196548708353\n",
      "0.43421965487083797\n",
      "0.43421965487084097\n",
      "0.4342196548708445\n",
      "0.43421965487084846\n",
      "0.43421965487085323\n",
      "0.43421965487085845\n",
      "0.43421965487086467\n",
      "0.4342196548708719\n",
      "0.43421965487088027\n",
      "0.43421965487089015\n",
      "0.43421965487090175\n",
      "0.4342196548709154\n",
      "0.43421965487093195\n",
      "0.4342196548709518\n",
      "0.4342196548709759\n",
      "0.43421965487100594\n",
      "0.43421965487104336\n",
      "0.4342196548710901\n",
      "0.43421965487115055\n",
      "0.43421965487122904\n",
      "0.434219654871332\n",
      "0.4342196548714686\n",
      "0.43421965487165165\n",
      "0.4342196548718876\n",
      "0.4342196548722199\n",
      "0.43421965487271486\n",
      "0.434219654873358\n",
      "0.43421965487424824\n",
      "0.43421965487548486\n",
      "0.4342196548772074\n",
      "0.4342196548796123\n",
      "0.43421965488297537\n",
      "0.43421965488768527\n",
      "0.43421965489428777\n",
      "0.43421965490355063\n",
      "0.4342196549165532\n",
      "0.4342196549348126\n",
      "0.43421965496046216\n",
      "0.4342196549964998\n",
      "0.43421965504713933\n",
      "0.43421965511830285\n",
      "0.4342196552101431\n",
      "0.43421965534966805\n",
      "0.43421965554255354\n",
      "0.4342196558131678\n",
      "0.43421965619278163\n",
      "0.4342196567252194\n",
      "0.43421965747188584\n",
      "0.43421965851880223\n",
      "0.43421965998644757\n",
      "0.4342196620435221\n",
      "0.43421966492618735\n",
      "0.43421966896495007\n",
      "0.4342196746222055\n",
      "0.43421968254463644\n",
      "0.4342196936362842\n",
      "0.43421970916035285\n",
      "0.43421973089084603\n",
      "0.43421976402322204\n",
      "0.43421981383760827\n",
      "0.43421987714717125\n",
      "0.43421996552936365\n",
      "0.4342200888136219\n",
      "0.4342202606019247\n",
      "0.4342204996439835\n",
      "0.4342208316432698\n",
      "0.43422129155597017\n",
      "0.43422192636829143\n",
      "0.43422279814338177\n",
      "0.43422398671271045\n",
      "0.43422559059696336\n",
      "0.4342277234405367\n",
      "0.43423091816407905\n",
      "0.43423458179694086\n",
      "0.4342390424115318\n",
      "0.4342440961601399\n",
      "0.4342496417874134\n",
      "0.43425518504521377\n",
      "0.4342602706670902\n",
      "0.43426455948025733\n",
      "0.4342679241380948\n",
      "0.43427043217972594\n",
      "0.43427225646307965\n",
      "0.43427352102717476\n",
      "0.4342745187812691\n",
      "0.4342753025511972\n",
      "0.4342759520348205\n",
      "0.4342765186621306\n",
      "0.43427239218946123\n",
      "0.43427285867598914\n",
      "0.4342758275311859\n",
      "0.4342762713479047\n",
      "0.43427753385905465\n",
      "0.4342779695092988\n",
      "0.43427579489581625\n",
      "0.43427621035630015\n",
      "0.4342766251229347\n",
      "0.4342770395857176\n",
      "0.4342774540297462\n",
      "0.43427786869458135\n",
      "0.4342782838161748\n",
      "0.4342786996610812\n",
      "0.4342814926212276\n",
      "0.43428192202473404\n",
      "0.43428398425974846\n",
      "0.43428441768279774\n",
      "0.43428485506282194\n",
      "0.4342854731083476\n",
      "0.4342859232286737\n",
      "0.4342885541874921\n",
      "0.43428903408845826\n",
      "0.43428953415677884\n",
      "0.43429006522178715\n",
      "0.434292322769012\n",
      "0.43429297466423206\n",
      "0.434293745006535\n",
      "0.43429470392533537\n",
      "0.4342959618874006\n",
      "0.43429768843110494\n",
      "0.43430013214833224\n",
      "0.4343036292434663\n",
      "0.43430856984580457\n",
      "0.43431527037841716\n",
      "0.4343237154467887\n",
      "0.4343332618883084\n",
      "0.43434261722614226\n",
      "0.43435039592651375\n",
      "0.43435584082766754\n",
      "0.43435905889704846\n",
      "0.43436014814565166\n",
      "0.4343606996489618\n",
      "0.4343606585707081\n",
      "0.4343602985751366\n",
      "0.43435977281638966\n",
      "0.43435916309273165\n",
      "0.4343585120359428\n",
      "0.43435784158062873\n",
      "0.4343516033285206\n",
      "0.4343511467169064\n",
      "0.4343504858253956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4343498269865729\n",
      "0.43434917091213204\n",
      "0.4343485179531086\n",
      "0.43434786827731475\n",
      "0.4343472219594233\n",
      "0.43434657902668583\n",
      "0.4343459394821304\n",
      "0.4343453033163314\n",
      "0.43434467051338077\n",
      "0.43434404105392027\n",
      "0.4343434149166825\n",
      "0.4343427920792753\n",
      "0.43434217251858476\n",
      "0.4343472136332685\n",
      "0.4343465814819963\n",
      "0.4343459524318525\n",
      "0.4343453264602997\n",
      "0.4343458049391446\n",
      "0.43434517561168845\n",
      "0.4343445492576078\n",
      "0.4343439258537912\n",
      "0.434343305376947\n",
      "0.4343426878036172\n",
      "0.4343420731101915\n",
      "0.4343414612729221\n",
      "0.4343408522679378\n",
      "0.43434024607125976\n",
      "0.43433964265881647\n",
      "0.4343390420064597\n",
      "0.4343383241854446\n",
      "0.4343377252832258\n",
      "0.43433750279637795\n",
      "0.4343369071564869\n",
      "0.43433631417979307\n",
      "0.43433572384236\n",
      "0.43433513612027375\n",
      "0.43433455098966084\n",
      "0.4343339684267055\n",
      "0.4343333884076686\n",
      "0.4343336548214128\n",
      "0.43433307582567615\n",
      "0.43433249927768874\n",
      "0.434331925154507\n",
      "0.4343313534333733\n",
      "0.4343305660744719\n",
      "0.4343299998560839\n",
      "0.434329435975658\n",
      "0.43432887441122797\n",
      "0.4343283151411017\n",
      "0.434327758143879\n",
      "0.43432720339846737\n",
      "0.43432665088409994\n",
      "0.4343261005803506\n",
      "0.43432555246715027\n",
      "0.43432500652480255\n",
      "0.43432446273399794\n",
      "0.4343239210758289\n",
      "0.4343233815318037\n",
      "0.4343228440838592\n",
      "0.43432230871437394\n",
      "0.43432177540618067\n",
      "0.43432124414257706\n",
      "0.43432071490733715\n",
      "0.4343201876847212\n",
      "0.4343196624594852\n",
      "0.43431913921688964\n",
      "0.4343186179427071\n",
      "0.43431809862322984\n",
      "0.43431758124527614\n",
      "0.4343170657961958\n",
      "0.43431655226387467\n",
      "0.43431604063673923\n",
      "0.4343155309037595\n",
      "0.4343154664976772\n",
      "0.43431494588603486\n",
      "0.4343144272938677\n",
      "0.43431391071396486\n",
      "0.4343133961396252\n",
      "0.43431288356465264\n",
      "0.4343123729833523\n",
      "0.43431186439052527\n",
      "0.43431135778146257\n",
      "0.4343087766622406\n",
      "0.43430829171842816\n",
      "0.4343078085311312\n",
      "0.4343073270985912\n",
      "0.43430684741954695\n",
      "0.43430636949322365\n",
      "0.43430589331932234\n",
      "0.43430541889800794\n",
      "0.434304946229897\n",
      "0.4343044753160452\n",
      "0.43430400615793335\n",
      "0.43430353875745487\n",
      "0.4343030731169012\n",
      "0.4343026092389474\n",
      "0.43430214712663734\n",
      "0.4343016867833694\n",
      "0.43430122821288036\n",
      "0.4343024830569698\n",
      "0.43430200601988705\n",
      "0.4343015311172675\n",
      "0.43430105835219807\n",
      "0.43430058772796215\n",
      "0.4343004473037712\n",
      "0.4342999731823327\n",
      "0.434299501262481\n",
      "0.43429876521981736\n",
      "0.43429808113394414\n",
      "0.4342976167511205\n",
      "0.4342971545652565\n",
      "0.43429669458149345\n",
      "0.43429623680501706\n",
      "0.4342957812410433\n",
      "0.4342953278948053\n",
      "0.4342945013143961\n",
      "0.4342940523441695\n",
      "0.4342936055711517\n",
      "0.4342931610005464\n",
      "0.43429271863753677\n",
      "0.4342922784872748\n",
      "0.4342918405548697\n",
      "0.4342916195249944\n",
      "0.4342902535714552\n",
      "0.4342898268881167\n",
      "0.4342894024449285\n",
      "0.4342889802456429\n",
      "0.43428856029393154\n",
      "0.434288142593376\n",
      "0.4342877271474605\n",
      "0.4342873139595645\n",
      "0.4342869030329554\n",
      "0.4342864943707818\n",
      "0.4342860879760672\n",
      "0.4342856838517036\n",
      "0.434285282000446\n",
      "0.43428488242490704\n",
      "0.4342844851275513\n",
      "0.43428409011069075\n",
      "0.43428369737648026\n",
      "0.4342833069269129\n",
      "0.43428291876381586\n",
      "0.43428253288884705\n",
      "0.4342821493034909\n",
      "0.43428176800905555\n",
      "0.43428138900666907\n",
      "0.434281012297277\n",
      "0.4342806378816397\n",
      "0.4342802657603291\n",
      "0.4342798959337271\n",
      "0.4342795284020234\n",
      "0.434279163165213\n",
      "0.4342788002230945\n",
      "0.4342784395752692\n",
      "0.4342781553730864\n",
      "0.4342777984803062\n",
      "0.4342774439330079\n",
      "0.4342779621935824\n",
      "0.43427760355772244\n",
      "0.4342772473863107\n",
      "0.4342768936750231\n",
      "0.43427654241933467\n",
      "0.43427577841042686\n",
      "0.4342754324843949\n",
      "0.43427508896098876\n",
      "0.4342747478354997\n",
      "0.4342744091030329\n",
      "0.4342740727585077\n",
      "0.4342733620252947\n",
      "0.4342730337009561\n",
      "0.43427270766195564\n",
      "0.4342723839041761\n",
      "0.43427206242329985\n",
      "0.43427174321481\n",
      "0.4342714262739922\n",
      "0.4342711115959346\n",
      "0.43427079917553063\n",
      "0.4342704890074799\n",
      "0.43427018108628995\n",
      "0.4342685362216645\n",
      "0.4342682364406529\n",
      "0.4342679389449708\n",
      "0.43426764372664167\n",
      "0.43426735077751877\n",
      "0.4342670600892884\n",
      "0.43426716045085395\n",
      "0.43426565848960597\n",
      "0.43426537960960704\n",
      "0.43426510303364263\n",
      "0.43426482874981087\n",
      "0.43426455674611364\n",
      "0.43426428701046027\n",
      "0.4342632333860565\n",
      "0.4342629720641633\n",
      "0.4342627128718753\n",
      "0.43426259942719514\n",
      "0.4342623444843419\n",
      "0.4342620917085438\n",
      "0.4342618410874169\n",
      "0.4342615926085064\n",
      "0.43426118714681816\n",
      "0.43426094266115745\n",
      "0.43426070028908775\n",
      "0.4342604600177021\n",
      "0.4342602218340417\n",
      "0.4342599857251002\n",
      "0.4342597516778258\n",
      "0.43425951967912596\n",
      "0.4342592897158693\n",
      "0.4342590617748895\n",
      "0.4342588358429888\n",
      "0.4342586119069405\n",
      "0.43425838995349225\n",
      "0.43425816996936967\n",
      "0.4342572489279038\n",
      "0.43425703464719034\n",
      "0.43425682228437323\n",
      "0.4342566118261514\n",
      "0.43425640325921655\n",
      "0.4342561965702564\n",
      "0.4342559917459572\n",
      "0.4342557887730065\n",
      "0.4342555876380957\n",
      "0.43425538832792276\n",
      "0.43425519082919517\n",
      "0.4342549951286315\n",
      "0.4342548012129652\n",
      "0.4342546090689456\n",
      "0.4342544186833416\n",
      "0.4342542300429429\n",
      "0.4342540431345631\n",
      "0.43425559304562805\n",
      "0.4342553591251398\n",
      "0.4342551731236477\n",
      "0.4342549888662641\n",
      "0.4342548063389903\n",
      "0.43425580648564316\n",
      "0.43425562788814737\n",
      "0.4342553149289636\n",
      "0.43425514195950254\n",
      "0.4342549706634175\n",
      "0.434254801026161\n",
      "0.43425463303326833\n",
      "0.43425446667035733\n",
      "0.4342543019231306\n",
      "0.4342541387773753\n",
      "0.4342539489418626\n",
      "0.4342537876754086\n",
      "0.4342536279835543\n",
      "0.43425346985226404\n",
      "0.4342550791722012\n",
      "0.4342549216827319\n",
      "0.4342547657403669\n",
      "0.4342546113311203\n",
      "0.43425445844110316\n",
      "0.4342543070565232\n",
      "0.43425415716368565\n",
      "0.4342531514166617\n",
      "0.43425181020885\n",
      "0.43425166890562217\n",
      "0.4342515289919513\n",
      "0.43425139045528977\n",
      "0.43425125328317843\n",
      "0.4342511174632462\n",
      "0.4342509530994099\n",
      "0.4342508205578306\n",
      "0.43425068933752325\n",
      "0.43425055942615626\n",
      "0.43425043081149695\n",
      "0.4342503034814111\n",
      "0.4342501774238628\n",
      "0.43425005262691496\n",
      "0.4342499290787287\n",
      "0.43424978985250806\n",
      "0.43424966934772863\n",
      "0.43424955005995136\n",
      "0.4342494319774983\n",
      "0.43424931508879566\n",
      "0.4342491993823731\n",
      "0.43424908484686314\n",
      "0.4342489714710016\n",
      "0.4342488592436258\n",
      "0.4342487481536748\n",
      "0.4342486381901893\n",
      "0.4342485293423101\n",
      "0.4342484215992781\n",
      "0.43424831495043403\n",
      "0.43424820938521724\n",
      "0.43424810489316573\n",
      "0.4342480591281039\n",
      "0.43424992730073103\n",
      "0.434249823220636\n",
      "0.43424972020531655\n",
      "0.43424961824441133\n",
      "0.4342495173276584\n",
      "0.4342494174448951\n",
      "0.434249318586057\n",
      "0.4342492207411775\n",
      "0.43424912390038717\n",
      "0.4342490280539123\n",
      "0.4342489331920751\n",
      "0.4342488393052925\n",
      "0.4342487463840752\n",
      "0.43424865441902755\n",
      "0.4342485634008463\n",
      "0.4342484733203201\n",
      "0.43424841960072014\n",
      "0.4342483321658687\n",
      "0.43424824563612757\n",
      "0.43424816000264943\n",
      "0.4342480752566761\n",
      "0.434247991389538\n",
      "0.43424790839265326\n",
      "0.43424826684209183\n",
      "0.43424818728720377\n",
      "0.4342481085665608\n",
      "0.43424803067196227\n",
      "0.4342479535952918\n",
      "0.4342478773285174\n",
      "0.43424780186369\n",
      "0.4342477271929431\n",
      "0.4342476533084915\n",
      "0.4342475802026311\n",
      "0.4342475078677373\n",
      "0.4342474362962652\n",
      "0.43424736548074794\n",
      "0.4342472954137963\n",
      "0.4342472260880981\n",
      "0.43424715749641707\n",
      "0.4342470896315922\n",
      "0.43424636370223996\n",
      "0.43424629659083314\n",
      "0.43424623019189085\n",
      "0.43424616449847664\n",
      "0.4342460995037258\n",
      "0.43424635570853126\n",
      "0.4342462930893248\n",
      "0.434246231139558\n",
      "0.434246169852742\n",
      "0.43424610922245493\n",
      "0.43424604924234206\n",
      "0.43424598990611424\n",
      "0.4342459312075478\n",
      "0.4342458731404834\n",
      "0.43424581569882603\n",
      "0.4342457588765437\n",
      "0.4342457026676671\n",
      "0.434245723791236\n",
      "0.4342456676680259\n",
      "0.43424561215092045\n",
      "0.43424655967797793\n",
      "0.434246504771932\n",
      "0.43424645046250027\n",
      "0.43424639674394666\n",
      "0.4342463436105955\n",
      "0.43424600613598485\n",
      "0.4342458384792835\n",
      "0.4342457893013599\n",
      "0.4342457406666823\n",
      "0.434245692570045\n",
      "0.4342456450062965\n",
      "0.43424559797033907\n",
      "0.43424555145712873\n",
      "0.43424550546167345\n",
      "0.43424545997903374\n",
      "0.4342454150043216\n",
      "0.43424537053269996\n",
      "0.4342453265593818\n",
      "0.4342452830796305\n",
      "0.4342447380884954\n",
      "0.434244696273651\n",
      "0.4342446549315212\n",
      "0.43424528112191774\n",
      "0.43424524110911017\n",
      "0.4342452015521377\n",
      "0.4342451624467087\n",
      "0.434245123788576\n",
      "0.4342450855735365\n",
      "0.43424504779742984\n",
      "0.4342450104561389\n",
      "0.4342449735455888\n",
      "0.4342449370617463\n",
      "0.43424490100061974\n",
      "0.43424486535825885\n",
      "0.43424483013075316\n",
      "0.4342447953142329\n",
      "0.4342447609048676\n",
      "0.43424472689886606\n",
      "0.43424469329247606\n",
      "0.43424466008198337\n",
      "0.434244627263712\n",
      "0.4342445948340233\n",
      "0.4342445627893159\n",
      "0.434244531126025\n",
      "0.43424449984062213\n",
      "0.4342444689296148\n",
      "0.4342444383895459\n",
      "0.43424440821699345\n",
      "0.4342443784085703\n",
      "0.4342443489609237\n",
      "0.4342443198707346\n",
      "0.4342442911347178\n",
      "0.43424426274962125\n",
      "0.4342442347122259\n",
      "0.4342442070193451\n",
      "0.43424417966782425\n",
      "0.43424415265454075\n",
      "0.4342441259764034\n",
      "0.43424409963035204\n",
      "0.4342444394524054\n",
      "0.4342444137632166\n",
      "0.43424438839714996\n",
      "0.4342443633512731\n",
      "0.4342443386226824\n",
      "0.43424431420850373\n",
      "0.43424429010589144\n",
      "0.43424405040678177\n",
      "0.43424402786718397\n",
      "0.43424400562149823\n",
      "0.43424398366708084\n",
      "0.43424396200131454\n",
      "0.43424394062160765\n",
      "0.4342439195253942\n",
      "0.4342438987101338\n",
      "0.4342438781733109\n",
      "0.4342437740297814\n",
      "0.43424375457256226\n",
      "0.43424373538128536\n",
      "0.43424371645358395\n",
      "0.4342436977871142\n",
      "0.43424367937955544\n",
      "0.4342436612286096\n",
      "0.434243643332001\n",
      "0.43424473579115996\n",
      "0.434244333292271\n",
      "0.4342443167815891\n",
      "0.4342443005129475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43424428448420266\n",
      "0.4342442686932323\n",
      "0.4342442531379346\n",
      "0.43424423781622823\n",
      "0.4342442227260519\n",
      "0.4342442078653647\n",
      "0.4342441932321452\n",
      "0.4342441788243917\n",
      "0.4342441646401219\n",
      "0.4342441506773727\n",
      "0.4342441369341999\n",
      "0.43424412340867796\n",
      "0.43424411009890035\n",
      "0.43424394960811763\n",
      "0.43424393667570765\n",
      "0.43424392395419104\n",
      "0.43424391144172053\n",
      "0.4342438991364668\n",
      "0.434243887036618\n",
      "0.43424387514037954\n",
      "0.4342430141664302\n",
      "0.43424300221645534\n",
      "0.43424299046669856\n",
      "0.43424297891541963\n",
      "0.4342429675608951\n",
      "0.4342429564014179\n",
      "0.4342429454352974\n",
      "0.43424293466085867\n",
      "0.43424292407644316\n",
      "0.4342429136804077\n",
      "0.43424290347112493\n",
      "0.43424289344698275\n",
      "0.4342428836063847\n",
      "0.43424287394774885\n",
      "0.4342428644695089\n",
      "0.4342428551701127\n",
      "0.4342428460480232\n",
      "0.4342428371017176\n",
      "0.43424282832968764\n",
      "0.4342428197304392\n",
      "0.43424281130249204\n",
      "0.43424280304438007\n",
      "0.4342427949546509\n",
      "0.43424278703186553\n",
      "0.4342427792745988\n",
      "0.43424277168143877\n",
      "0.43424276425098646\n",
      "0.43424275698185627\n",
      "0.43424274987267564\n",
      "0.43424126436942706\n",
      "0.4342412579298145\n",
      "0.43424125164010297\n",
      "0.4342412454990183\n",
      "0.434241239505298\n",
      "0.43424123365769185\n",
      "0.4342410523848682\n",
      "0.4342410468867234\n",
      "0.4342410415301374\n",
      "0.434240914838512\n",
      "0.43424090969674606\n",
      "0.43424090469344995\n",
      "0.43424089982745534\n",
      "0.4342408950976048\n",
      "0.43424089050275133\n",
      "0.43424088604175914\n",
      "0.43424088171350217\n",
      "0.4342408775168651\n",
      "0.4342408734507428\n",
      "0.4342409496843177\n",
      "0.4342408579620797\n",
      "0.43424085415043423\n",
      "0.43424085046568484\n",
      "0.4342408469067711\n",
      "0.434240843472642\n",
      "0.4342414699243381\n",
      "0.43424104718202317\n",
      "0.4342410443890672\n",
      "0.43424104171591077\n",
      "0.43424103916155415\n",
      "0.43424103672500625\n",
      "0.43424103440528533\n",
      "0.43424045000544753\n",
      "0.434240447977666\n",
      "0.4342404460620477\n",
      "0.43424044425766034\n",
      "0.43424051015563153\n",
      "0.4342405085350475\n",
      "0.4342405070235155\n",
      "0.4342405979287758\n",
      "0.43424059666366815\n",
      "0.43424059550481026\n",
      "0.4342405944513214\n",
      "0.43424059350232913\n",
      "0.43424059265696885\n",
      "0.43424059191438347\n",
      "0.43424059127372416\n",
      "0.4342405907341492\n",
      "0.43424059029482487\n",
      "0.4342405899549247\n",
      "0.4342405897136297\n",
      "0.4342405895701284\n",
      "0.43424058952361644\n",
      "0.4342405895732967\n",
      "0.4342405897183793\n",
      "0.4342405899580812\n",
      "0.4342408350765026\n",
      "0.434240914711987\n",
      "0.4342409151797381\n",
      "0.43424091574003093\n",
      "0.43423975067325765\n",
      "0.4342389153172328\n",
      "0.43423891580182233\n",
      "0.43423891637633893\n",
      "0.4342389170400419\n",
      "0.4342388702095538\n",
      "0.4342388711856589\n",
      "0.4342388722474481\n",
      "0.43423887339421835\n",
      "0.43423887462527283\n",
      "0.434238875939921\n",
      "0.43423887733747835\n",
      "0.4342388788172669\n",
      "0.4342388803786141\n",
      "0.4342388820208536\n",
      "0.4342388837433251\n",
      "0.4342385486382913\n",
      "0.4342385504212311\n",
      "0.43423855228312097\n",
      "0.4342385542233181\n",
      "0.4342385562411848\n",
      "0.4342385583360894\n",
      "0.4342385605074057\n",
      "0.4342385627545127\n",
      "0.4342385650767956\n",
      "0.43423856747364403\n",
      "0.4342385699444537\n",
      "0.4342385724886255\n",
      "0.4342385751055656\n",
      "0.43423857779468505\n",
      "0.43423858055540043\n",
      "0.4342385833871333\n",
      "0.4342385862893102\n",
      "0.4342385892613629\n",
      "0.43423859230272777\n",
      "0.4342385954128465\n",
      "0.43423864029382836\n",
      "0.4342386435176149\n"
     ]
    }
   ],
   "source": [
    "likelihood = 0\n",
    "old_likelihood = -np.inf\n",
    "iters = 0\n",
    "max_iters = 1000\n",
    "max_epocas_gating = 1\n",
    "alpha_gating = 0.5\n",
    "while abs(likelihood-old_likelihood) > 1e-3 and iters < max_iters:\n",
    "    iters += 1\n",
    "    #calcula a saida para cada rede\n",
    "    #A funcao retorna 4 varaiveis, queremos apenas a ultima que representa o output (por isso o [-1])\n",
    "    output_gating = gating_network.feedforward(X_train)[-1]\n",
    "    output_experts = np.matrix([np.ravel(expert.feedforward(X_train)[-1]).tolist() for expert in all_experts]).T\n",
    "    #Agora que temos a saida comecamos com a funcao de EM\n",
    "    old_likelihood = likelihood\n",
    "    #Passo E (Expectation)\n",
    "    #Com os parametros atuais calculamos calculamos a 'expectation' posterior para cada expert\n",
    "    likelihood,haux_train = calcula_verossimilhanca(num_experts, y_train, output_experts, output_gating)\n",
    "    #likelihood_val,haux_val = calcula_verossimilhanca(num_experts, y_train, output_experts, output_gating, matriz_covariancia)\n",
    "    #h = haux./(sum(haux,2)*ones(1,m));\n",
    "    h = np.divide(haux_train, np.sum(haux_train,axis=0))\n",
    "    #Passo M (Maximizacao)\n",
    "    maximiza_gating(gating_network,max_epocas_gating, alpha_gating, X_train, h, X_val, y_val)\n",
    "    #Itera por cada expert o treinando com seu respectivo h\n",
    "    for exp_index, expert in enumerate(all_experts):\n",
    "        output_individual_exp = []\n",
    "        for item in h[:,0]:\n",
    "            output_individual_exp.append([item])\n",
    "        maximiza_expert(expert,max_epocas_gating,alpha_gating, X_train, output_individual_exp, X_val, y_val)\n",
    "    saida_final = output_final(X_train, gating_network, all_experts)\n",
    "    erro = saida_final - y_train \n",
    "    loss = np.square(erro).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "teste = output_final(X_train, gating_network, all_experts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01156333 0.01042883 0.00938287 0.00813877 0.00709063 0.00956672\n",
      " 0.00672117 0.0080347  0.00572201 0.00948199 0.00769875 0.01884049\n",
      " 0.01390172 0.02154663 0.01157301 0.01491996 0.01071675 0.01509907\n",
      " 0.01033117 0.01835214 0.01107813 0.02250479 0.01534226 0.02272517\n",
      " 0.01751616 0.01918316 0.01437816 0.01984835 0.01598412 0.02016663\n",
      " 0.01729386 0.01934895 0.01332784 0.01959735 0.0113897  0.01996717\n",
      " 0.01217667 0.0185082  0.01135169 0.01939163 0.01446376 0.02311653\n",
      " 0.0156179  0.02507302 0.01956827 0.02406632 0.02158657 0.01886385\n",
      " 0.02262253 0.01986753 0.02420045 0.02143355 0.01881815 0.02151411\n",
      " 0.01867279 0.01819353 0.01514594 0.02153682 0.01460541 0.01976641\n",
      " 0.01136766 0.01970821]\n"
     ]
    }
   ],
   "source": [
    "print(np.array(h[:,0]).T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import sys\n",
    "\n",
    "#import Perceptron_Regressor\n",
    "#import Utilities\n",
    "\n",
    "import importlib\n",
    "importlib.reload(Perceptron_Regressor)\n",
    "# importlib.reload(Utilities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_likelihood(d, y_experts, y_g, covar_matrix):\n",
    "    #list with the probablities of all errors from each expert to each line of input\n",
    "    py_matrix = []\n",
    "    #iterate over the experts\n",
    "    for exp in range(y_experts[0].shape[1]):\n",
    "        py_row = []\n",
    "        #for each expert, calculate the individual py (sum of logs of final output - the gating output multiplied by the corresponding expert output)\n",
    "        for inst, value in enumerate(d):\n",
    "            #diff is the error of the final output\n",
    "            diff = value-y_experts[inst,exp]\n",
    "            py = np.exp(-np.dot(diff, diff.T) / np.multiply(2, covar_matrix[exp,exp]))\n",
    "            py_row += [py[0,0]]\n",
    "        py_matrix += [py_row]\n",
    "    \n",
    "    py_matrix = np.matrix(py_matrix).T\n",
    "    h_aux = np.multiply(y_g, py_matrix)\n",
    "    likelihood = np.sum(np.log(np.sum(h_aux, axis=1)))\n",
    "    \n",
    "    return likelihood, h_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#util = Utilities.Utilities()\n",
    "\n",
    "# def calc_likelihood(d, y_experts, y_g, covar_matrix):\n",
    "#     #create the matrix with the errors of each expert to each instance\n",
    "#     diff = d-y_experts\n",
    "    \n",
    "#     #create the auxiliar matrix that computes the squared error for each expert,\n",
    "#     # multiplied by the covariance matrix,\n",
    "#     # Note: in this case we use the Identity for the covariance matrix\n",
    "#     expert_error = np.sum(np.multiply(covar_matrix, np.dot(diff.T, diff)), axis=0)\n",
    "\n",
    "#     # calculates py as the prior probability multiplied by the exp \n",
    "#     # of the expert_error multiplied by -0.5\n",
    "#     py = np.multiply(np.exp(np.multiply(expert_error, -0.5)), y_g)\n",
    "\n",
    "#     #calculates the final likelihood, computing the sum for each instance of the logs of py\n",
    "#     ll = -np.sum(np.log(py))\n",
    "    \n",
    "#     return ll, py\n",
    "\n",
    "def calc_2norm(m):\n",
    "    return np.sqrt(np.sum(np.multiply(m,m)))\n",
    "\n",
    "def maximize_gating(gating_net, X_train, h, max_it = 1e4, min_norm = 1e-5, X_valid=None, d_valid=None, use_fit=False):\n",
    "    if use_fit:\n",
    "        gating_net.fit(X=X_train, d=h, valid_data=X_valid, valid_d=d_valid, verbose=False)\n",
    "    else:\n",
    "        norm_grad = float(\"inf\")\n",
    "        it = 0\n",
    "\n",
    "        while norm_grad > min_norm and it < max_it:\n",
    "            print(it, norm_grad)\n",
    "            #calculate the descent gradient for h\n",
    "            djdw1, djdw2 = gating_net.calculate_gradient(X=X_train, d=h)\n",
    "            #compute the right leraning rate\n",
    "            learn_rate = gating_net.calculate_bisection(X=X_train, d=h, djdw1=djdw1, djdw2=djdw2)\n",
    "            #learn_rate=0.1\n",
    "            #update the gating network wheights\n",
    "            gating_net.w1, gating_net.w2 = gating_net.update_weights(learning_rate=learn_rate, djdw1=djdw1, djdw2=djdw2,\n",
    "                                                                     w1=gating_net.w1, w2=gating_net.w2)\n",
    "            it+=1\n",
    "            norm_grad = calc_2norm(np.append(djdw1.ravel(), djdw2.ravel(), axis=1))\n",
    "\n",
    "def maximize_expert(expert_net, X_train, h, d_train, covar_matrix=None, max_it = 1e4, min_norm = 1e-5, X_valid=None, d_valid=None, use_fit=False):\n",
    "    if use_fit:\n",
    "        expert_net.fit(X=X_train, d=d_train, valid_data=X_valid, valid_d=d_valid, side_factor=h, verbose=False)\n",
    "    else:\n",
    "        norm_grad = float(\"inf\")\n",
    "        it = 0\n",
    "\n",
    "        #print(\"expert start new\")\n",
    "        while norm_grad > min_norm and it < max_it:\n",
    "            #calculate the descent gradient for h\n",
    "            #print(\"expert start\")\n",
    "            djdw1, djdw2 = gating_net.calculate_gradient(X=X_train, d=d_train, side_factor=h)\n",
    "            #print(\"grad\")\n",
    "            #compute the right leraning rate\n",
    "            learn_rate = gating_net.calculate_bisection(X=X_train, d=d_train, djdw1=djdw1, djdw2=djdw2, side_factor=h)\n",
    "            #learn_rate=0.1\n",
    "            #print(\"alfa\")\n",
    "            #update the gating network wheights\n",
    "            gating_net.w1, gating_net.w2 = gating_net.update_weights(learning_rate=learn_rate, djdw1=djdw1, djdw2=djdw2,\n",
    "                                                                     w1=gating_net.w1, w2=gating_net.w2)\n",
    "\n",
    "            it+=1\n",
    "            norm_grad = calc_2norm(np.append(djdw1.ravel(), djdw2.ravel(), axis=1))\n",
    "\n",
    "            #print(\"weights\")\n",
    "\n",
    "def calc_final_pred(X, gating_net, experts_list):\n",
    "    y_g = gating_net.forward(X)\n",
    "\n",
    "    #y_e = []\n",
    "    #for exp in experts_list:\n",
    "    #    y_e +=[exp.forward(X).T[0]]\n",
    "    #    print()\n",
    "    #print(y_e)\n",
    "    #y_e =np.matrix(y_e).T\n",
    "    y_e = np.matrix([np.array(exp.forward(X).T.tolist()[0]) for exp in experts_list]).T\n",
    "\n",
    "    return np.sum(np.multiply(y_e, y_g), axis=1)\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    # In[ ]:\n",
    "    input_path = r\"input\\treinamento-1.txt\"\n",
    "\n",
    "    input_data=pd.read_csv(open(input_path, \"r\"), header=None)\n",
    "    input_data.columns = [\"time_series\"]\n",
    "    input_data.head()\n",
    "\n",
    "\n",
    "    # In[17]:\n",
    "\n",
    "\n",
    "    lagged_data = util.create_lags(input_data, n=20)\n",
    "\n",
    "    #util.to_excel(lagged_data, \"time_series_with_lag.xlsx\")\n",
    "\n",
    "\n",
    "    # In[18]:\n",
    "\n",
    "\n",
    "    correlations = lagged_data.corr()\n",
    "\n",
    "    #util.to_excel(correlations, \"correlations.xlsx\")\n",
    "\n",
    "\n",
    "    # In[19]:\n",
    "\n",
    "\n",
    "    # CRIA OS INPUTS\n",
    "    max_lags = 5\n",
    "\n",
    "    lagged_data_ = lagged_data[[\"time_series\"] + [\"time_series_lag_{}\".format(i) for i in range(1, max_lags+1)]].dropna()\n",
    "    test, train = util.get_simple_sample(lagged_data_, 0.7)\n",
    "    valid, test = util.get_simple_sample(test, 0.5)\n",
    "\n",
    "    X_train= train[[\"time_series_lag_{}\".format(i) for i in range(1, max_lags+1)]]\n",
    "    X_train[\"bias\"] = 1 # adicionando bias\n",
    "    d_train = train[[\"time_series\"]]\n",
    "\n",
    "    X_test = test[[\"time_series_lag_{}\".format(i) for i in range(1, max_lags+1)]]\n",
    "    X_test[\"bias\"] = 1 # adicionando bias\n",
    "    d_test = test[[\"time_series\"]]\n",
    "\n",
    "    X_valid = valid[[\"time_series_lag_{}\".format(i) for i in range(1, max_lags+1)]]\n",
    "    X_valid[\"bias\"] = 1 # adicionando bias\n",
    "    d_valid = valid[[\"time_series\"]]\n",
    "\n",
    "    nInp = len(X_train.loc[0])\n",
    "    nOut = len(d_train.loc[0])\n",
    "    nHid_gat = 3\n",
    "    nHid_exp = 3\n",
    "    nExperts = 4\n",
    "\n",
    "    #creates the gating network\n",
    "    gating_net = Perceptron_Regressor.MLP(nInp=nInp, nHid=nHid_gat, nOut=nExperts,\n",
    "                                          fFunc=\"sigmoid\", gFunc=\"softmax\",\n",
    "                                          cost_func=\"entropy\")\n",
    "\n",
    "    #creates the expert networks list\n",
    "    experts_list = []\n",
    "    for i in range(nExperts):\n",
    "        expert = Perceptron_Regressor.MLP(nInp=nInp, nHid=nHid_gat, nOut=nOut,\n",
    "                                          fFunc=\"sigmoid\", gFunc=\"sigmoid\",\n",
    "                                          cost_func=\"mse\")\n",
    "        \n",
    "        experts_list+=[expert]\n",
    "\n",
    "\n",
    "    likelihood = 0\n",
    "    likelihood_prev = -float(\"inf\")\n",
    "    max_iterations = 1000\n",
    "    min_ll_gain = 1e-3\n",
    "    covar_matrix = np.identity(nExperts)\n",
    "\n",
    "    #loop to execute the Expectation Maximization algorithm\n",
    "    it = 0\n",
    "    while it < max_iterations and abs(likelihood-likelihood_prev) > min_ll_gain:\n",
    "    #     st_time = time.time()\n",
    "        it+=1\n",
    "        #calculates the outputs of each network\n",
    "        y_g = gating_net.forward(X_train)\n",
    "        #y_experts = np.matrix([exp.forward(X_train).T[0] for exp in experts_list]).T\n",
    "        y_experts = np.matrix([np.array(exp.forward(X_train).T.tolist()[0]) for exp in experts_list]).T\n",
    "        \n",
    "        y_g_valid = gating_net.forward(X_valid)\n",
    "        #y_experts_valid = np.matrix([exp.forward(X_valid).T[0] for exp in experts_list]).T\n",
    "        y_experts_valid = np.matrix([np.array(exp.forward(X_valid).T.tolist()[0]) for exp in experts_list]).T\n",
    "        \n",
    "        #E step - Expectation\n",
    "        # calculates the matrix h of posterior expectations for each expert\n",
    "        likelihood_prev = likelihood\n",
    "        likelihood, h_aux = calc_likelihood(d=np.matrix(d_train), y_experts=y_experts,\n",
    "                                            y_g=y_g, covar_matrix=covar_matrix)\n",
    "        likelihood_valid, h_aux_valid = calc_likelihood(d=np.matrix(d_valid), y_experts=y_experts_valid,\n",
    "                                            y_g=y_g_valid, covar_matrix=covar_matrix)\n",
    "    #     print(\"Likelihood time =\", time.time()-st_time)\n",
    "        \n",
    "    #     st_time = time.time()\n",
    "        #computes the h (posteriori likelihood) dividing elementwise the h_aux\n",
    "        # by the sum of all elements of the matrix \n",
    "        h = np.divide(h_aux, np.sum(h_aux, axis=1))\n",
    "        h_valid = np.divide(h_aux_valid, np.sum(h_aux_valid, axis=1))\n",
    "        \n",
    "        #M step - Maximization\n",
    "        # minimize the cost function for gating and expert networks (maximize the ouputs)\n",
    "        #First - maximize gating network (calulate the descend gradient for the error to h)\n",
    "        maximize_gating(gating_net=gating_net, X_train=X_train, h=h,\n",
    "                        X_valid=X_valid, d_valid=d_valid,use_fit=True)\n",
    "    #     print(\"Maximixing Gating time =\", time.time()-st_time)\n",
    "        \n",
    "        #then maximize each of the experts\n",
    "        for k, expert in enumerate(experts_list):\n",
    "    #         st_time = time.time()\n",
    "            #compute the expert responsability in the error for each instance\n",
    "            #expert_responsability = np.multiply(d_train, np.divide(np.sum(h, axis=0)[0,k], covar_matrix[k,k]))        \n",
    "            maximize_expert(expert_net=expert, X_train=X_train, h=h[:,k],\n",
    "                            d_train=d_train, covar_matrix=covar_matrix,\n",
    "                            X_valid=X_valid, d_valid=d_valid,use_fit=True)\n",
    "    #         print(\"\\t\\tMaximazing Expert\",k,\"time =\", time.time()-st_time)\n",
    "            \n",
    "        y_pred = calc_final_pred(X=X_train, gating_net=gating_net, experts_list=experts_list)\n",
    "        mse = util.get_mse(y_pred, d_train)\n",
    "        print(it, \"\\t\", likelihood, \"\\t\", mse)\n",
    "    print(\"y_pred:\\n\", y_pred)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
